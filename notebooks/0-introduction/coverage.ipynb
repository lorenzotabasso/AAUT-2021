{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = lr.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for evaluating fp,tp and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fp_tp(actual, predicted):\n",
    "    fp = np.logical_and(predicted, np.logical_not(actual))\n",
    "    tp = np.logical_and(predicted, actual)\n",
    "    return (sum(fp), sum(tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(actual, predicted):\n",
    "    tp = np.logical_and(predicted, actual)\n",
    "    tn = np.logical_and(np.logical_not(predicted), np.logical_not(actual))\n",
    "    return (sum(tp) + sum(tn))/len(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fp,tp and accuracy evaluations for different thresholds\n",
    "\n",
    "Given the scores for the test cases, we might want to find the best possible threshold for classification, i.e., the real value $t$ such that `scores >` $t$ gives the best classifiation of the examples. \n",
    "\n",
    "Let us then start to consider 100 possible thresholds in the range $[0,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [i/100.0 for i in range(0,100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compute the tp, fp, and accuracy values of the labelings obtained by comparing the scores with those thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = y_test\n",
    "performances = []\n",
    "fps, tps = [], []\n",
    "for t in thresholds:\n",
    "    predicted = scores > t\n",
    "    fp, tp = eval_fp_tp(actual, predicted)\n",
    "    fps.append(fp)\n",
    "    tps.append(tp)\n",
    "\n",
    "    performances.append((eval_accuracy(actual, predicted), t, fp, tp))\n",
    "\n",
    "performances = np.array(performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "\n",
    "Let us then start plotting the coverage plot for the obtained classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fps, tps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking performances for threshold 0.5\n",
    "\n",
    "The predict_proba method we used to get the score returns the probability that examples belong to the positive class. Usually the positive class is then predicted as score > 0.5 (since in this case it is the one the largest likelihood).\n",
    "\n",
    "Let's then see where this classifier (i.e., the one obtained setting the threshold to 0.5) lays in the coverage plot and if there are better options.\n",
    "\n",
    "**note**: since we saved interesting stats in the `performances` array, we can retrieve the fp, tp position of the classifier we get by setting the thresholds to 0.5, by finding the position of the row we are interested using the expression: `performances[:,1] == 0.5` and then using the resulting boolean vector to retrieve the correct row of the matrix: `performances[performances[:,1] == 0.5]`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fps, tps)\n",
    "accuracy, threshold, fp, tp = performances[performances[:,1] == 0.5][0]\n",
    "plt.scatter(fp,tp,color='red')\n",
    "plt.plot([fp-10,fp+10],[tp-10,tp+10], color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is shown by the red dot and the red line, threshold 0.5 is a good one, but apparently two other points can reach a better classification.\n",
    "\n",
    "Let us see where these point lay in the plot and what is their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf05 = performances[performances[:, 1] == 0.5][0,0] # the subscription returns a matrix with a single row, but still two dimensions, we need to get the element in the first position of that matrix...\n",
    "\n",
    "performances[performances[:,0] > perf05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two points that we are looking for are then in position (5,121) and (1,117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fps, tps)\n",
    "fp, tp = eval_fp_tp(actual, scores > 0.5)\n",
    "plt.scatter(fp,tp, color=\"red\")\n",
    "plt.scatter(5,121, color=\"orange\")\n",
    "plt.scatter(1,117, color=\"orange\")\n",
    "plt.plot([fp-10,fp+10],[tp-10,tp+10], color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two points (that we found by looking only to the accuracies) are indeed the two points that the plot show having a better accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
