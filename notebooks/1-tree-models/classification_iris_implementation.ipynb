{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Classifiers introduction - Exercise implementation\n",
    "\n",
    "## Your work: what you have to do\n",
    "Modify the given Jupyter notebook on decision trees on Iris data and perform the following tasks:\n",
    "\n",
    "1. get an artificial inflation of some class in the training set by a given factor: 10 (weigh more the classes virginica e versicolor which are more difficult to discriminate). Learn the tree in these conditions.\n",
    "2. modify the weight of some classes (set to 10 the weights for misclassification between virginica into versicolor and vice versa) and learn the tree in these conditions. You should obtain similar results as for step 1.\n",
    "3. learn trees but try to avoid overfitting (by improving the error on the test set) tuning the hyper-parameters on: the minimum number of samples per leaf, max depth of the tree, min_impurity_decrease parameters, max leaf nodes, etc. Use misclassification error.\n",
    "4. build the confusion matrix of the created tree models on the test set and show them. \n",
    "5. build the ROC curves (or coverage curves in coverage space) and plot them for each tree model you have created: for each model you have to build three curves, one for each class, considered in turn as the positive class. (1-vs-rest model)\n",
    "\n",
    "In order to implement the fifth step, youi have 2 altenatives: \n",
    "\n",
    "1. Implement yourself the function which you need.\n",
    "2. In the sklearn package, there is thew empirical probability. Decision tree can be set to compute probability for classify each class. In this case the teacher may ask you to explain which function you have use. (clf.predict_proba - Probability prediction foreach classifier)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Global imports for this notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Constants\n",
    "random_state=300  # Used in the decision tree classifier"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Step 1\n",
    "\n",
    "Get an artificial inflation of some class in the training set by a given factor: 10 (weigh more the classes virginica e versicolor which are more difficult to discriminate). Learn the tree in these conditions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "inflation_factor = 10\n",
    "inflated_iris = dict()\n",
    "\n",
    "print(\"The three classes are: {}\\n\".format(iris.target_names))\n",
    "\n",
    "# Since the only class which we don't want to inflate is setosa (value 0), \n",
    "# we can write this single line to obtain an array specifying\n",
    "# how many times each element of class virginica and versicolor has to be repeated \n",
    "keep = np.clip(iris.target * inflation_factor, a_min=1, a_max=inflation_factor)\n",
    "print(\"len(keep)={0}\\n\".format(len(keep)))\n",
    "\n",
    "inflated_iris['data'] = np.repeat(iris.data, keep, axis=0)\n",
    "inflated_iris['target'] = np.repeat(iris.target, keep, axis=0)\n",
    "print(\"len(inflated_iris)={0}\\n\".format(len(inflated_iris[\"data\"])))\n",
    "# 50 setosa, 50 * 10 (500) versicolor and 50 * 10 (500) virginica. Total of 1050.\n",
    "\n",
    "# Shuffle the data\n",
    "#np.random.seed(0)\n",
    "#indices = np.random.permutation(len(inflated_iris['data']))  \n",
    "# \"indices\" is an array containing all the index of \"inflated_iris['data']\" in random order\n",
    "\n",
    "# We keep the last 210 indices (20%) for test set, the remaining for the training set\n",
    "#indices_training=indices[:-210]\n",
    "#indices_test=indices[-210:]\n",
    "\n",
    "# training on the first 840 elements\n",
    "#X_train_inf = inflated_iris['data'][indices_training]\n",
    "#y_train_inf = inflated_iris['target'][indices_training]\n",
    "\n",
    "# testing on the the last 210 elements\n",
    "#X_test_inf  = inflated_iris['data'][indices_test]\n",
    "#y_test_inf  = inflated_iris['target'][indices_test]\n",
    "\n",
    "# Split arrays into random train and test subsets\n",
    "# Note: \"random_state\" is a seed input for a number generator.\n",
    "X_train_inf, X_test_inf, y_train_inf, y_test_inf = train_test_split(inflated_iris['data'], inflated_iris['target'], test_size=0.20, random_state=6)\n",
    "\n",
    "# fit the model to the training data\n",
    "clf_inflated = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=random_state,min_samples_leaf=5)\n",
    "clf_inflated = clf_inflated.fit(X_train_inf, y_train_inf)\n",
    "\n",
    "# apply fitted model \"clf\" to the test set \n",
    "predicted_y_test_inf = clf_inflated.predict(X_test_inf)\n",
    "\n",
    "# print the predictions (class numbers associated to classes names in target names)\n",
    "print(\"Predictions:\\n{0} Length: {1}\\n\".format(predicted_y_test_inf, len(predicted_y_test_inf)))\n",
    "print(\"True classes:\\n{0} Length: {1}\".format(y_test_inf, len(y_test_inf)))\n",
    "\n",
    "truth_table = predicted_y_test_inf==y_test_inf\n",
    "\n",
    "print(\"\\nAre the arrays Predictions and True classes the same? {}\".format((truth_table).all()))\n",
    "print(\"{0} Length: {1}\".format(truth_table, len(y_test_inf)))\n",
    "print(\"True: {0} False:{1}\".format(sum(truth_table), len(y_test_inf) - sum(truth_table)))"
   ]
  },
  {
   "source": [
    "### Scoring and graph\n",
    "\n",
    "Printing final scores and tree."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "acc_score_inflated = accuracy_score(y_test_inf, predicted_y_test_inf)\n",
    "print(\"Accuracy score:\\t{}\".format(acc_score_inflated))\n",
    "\n",
    "# The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "# where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "f1_inflated = f1_score(y_test_inf, predicted_y_test_inf, average='macro')\n",
    "print(\"F1 score:\\t{}\".format(f1_inflated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "# printing final tree\n",
    "dot_data_w = tree.export_graphviz(clf_inflated, out_file=None,\n",
    "                         feature_names=iris.feature_names,\n",
    "                         class_names=iris.target_names,\n",
    "                         filled=True, rounded=True,\n",
    "                         special_characters=True)\n",
    "graph = graphviz.Source(dot_data_w)\n",
    "graph"
   ]
  },
  {
   "source": [
    "## Step 2\n",
    "\n",
    "Modify the weight of some classes (set to 10 the weights for misclassification between virginica into versicolor and vice versa) and learn the tree in these conditions. You should obtain similar results as for step 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Note: decreasing min_samples_leaf both the accuracy and F1 score are better! (the default for min_samples_leaf is 1)\n",
    "clf_weighted = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=random_state,min_samples_leaf=5,class_weight={0:1,1:10,2:10})\n",
    "\n",
    "# Load iris data\n",
    "X,y = load_iris(return_X_y=True)\n",
    "\n",
    "# Random state is for reproducing the same test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=7)\n",
    "# 20% of 150 is 30.\n",
    "\n",
    "# fit the model to the original training data\n",
    "clf_weighted = clf_weighted.fit(X_train, y_train)\n",
    "\n",
    "# apply fitted model \"clf_weighted\" to the test set \n",
    "predicted_y_test_w = clf_weighted.predict(X_test)\n",
    "\n",
    "# print the predictions (class numbers associated to classes names in target names)\n",
    "print(\"Predictions:\\n{0} Length: {1}\\n\".format(predicted_y_test_w, len(predicted_y_test_w)))\n",
    "print(\"True classes:\\n{0} Length: {1}\".format(y_test, len(y_test)))\n",
    "\n",
    "truth_table = predicted_y_test_w==y_test\n",
    "\n",
    "print(\"\\nAre the arrays Predictions and True classes the same? {}\".format((truth_table).all()))\n",
    "print(\"{0} Length: {1}\".format(truth_table, len(y_test)))\n",
    "print(\"True: {0} False:{1}\".format(sum(truth_table), len(y_test) - sum(truth_table)))"
   ]
  },
  {
   "source": [
    "### Scoring and graph\n",
    "\n",
    "Printing final scores and tree."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "acc_score = accuracy_score(y_test, predicted_y_test_w)\n",
    "print(\"Accuracy score:\\t{}\".format(acc_score))\n",
    "\n",
    "# The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "# where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "f1 = f1_score(y_test, predicted_y_test_w, average='macro')\n",
    "print(\"F1 score:\\t{}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "dot_data_w = tree.export_graphviz(clf_weighted, out_file=None,\n",
    "                         feature_names=iris.feature_names,\n",
    "                         class_names=iris.target_names,\n",
    "                         filled=True, rounded=True,\n",
    "                         special_characters=True)\n",
    "graph = graphviz.Source(dot_data_w)\n",
    "graph"
   ]
  },
  {
   "source": [
    "## Step 3\n",
    "\n",
    "Learn trees but try to avoid overfitting (by improving the error on the test set) tuning the hyper-parameters on: the minimum number of samples per leaf, max depth of the tree, min_impurity_decrease parameters, max leaf nodes, etc. Use misclassification error."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "from sklearn import tree \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We create a different DecionTree with tuned params class_weight={0:1,1:1,2:1}\n",
    "clf_refined = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=random_state,\n",
    "                                        min_samples_leaf=10, min_impurity_split=0.1, max_leaf_nodes= 10, min_samples_split=20, min_impurity_decrease=0.05)\n",
    "\n",
    "# fit the new model to the original training data\n",
    "clf_refined = clf_refined.fit(X_train, y_train)\n",
    "\n",
    "# apply fitted model \"clf\" to the test set \n",
    "predicted_y_test_ref = clf_refined.predict(X_test)\n",
    "\n",
    "# print the predictions (class numbers associated to classes names in target names)\n",
    "# print(iris.target_names)\n",
    "#print(\"\\nPredictions:\")\n",
    "#print(predicted_y_test_ref)\n",
    "#print(\"\\nTrue classes:\")\n",
    "#print(y_test) \n",
    "\n",
    "# print the predictions (class numbers associated to classes names in target names)\n",
    "print(\"Predictions:\\n{0} Length: {1}\\n\".format(predicted_y_test_ref, len(predicted_y_test_ref)))\n",
    "print(\"True classes:\\n{0} Length: {1}\".format(y_test, len(y_test)))\n",
    "\n",
    "truth_table = predicted_y_test_ref==y_test\n",
    "\n",
    "print(\"\\nAre the arrays Predictions and True classes the same? {}\".format((truth_table).all()))\n",
    "print(\"{0} Length: {1}\".format(truth_table, len(y_test)))\n",
    "print(\"True: {0} False:{1}\".format(sum(truth_table), len(y_test) - sum(truth_table)))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Scoring and graph\n",
    "\n",
    "Printing final scores and tree."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "acc_score_ref = accuracy_score(y_test, predicted_y_test_ref)\n",
    "print(\"Accuracy score:\\t{}\".format(acc_score_ref))\n",
    "\n",
    "# The F1 score is the macro avg between recall and precison\n",
    "f1_ref = f1_score(y_test, predicted_y_test_ref, average='macro') \n",
    "print(\"F1 score:\\t{}\".format(f1_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(clf_refined, out_file=None,\n",
    "                         feature_names=iris.feature_names,\n",
    "                         class_names=iris.target_names,\n",
    "                         filled=True, rounded=True,\n",
    "                         special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "source": [
    "## Step 4\n",
    "\n",
    "Build the confusion matrix of the created tree models on the test set and show them.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def display_side_by_side(dfs:list, captions:list):\n",
    "    \"\"\" Display tables side by side to save vertical space.\n",
    "    Source: https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side\n",
    "    Input:\n",
    "        dfs: list of pandas.DataFrame\n",
    "        captions: list of table captions\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    combined = dict(zip(captions, dfs))\n",
    "    for caption, df in combined.items():\n",
    "        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n",
    "        output += \"\\xa0\\xa0\\xa0\"\n",
    "    print()  # for prettier output \n",
    "    display(HTML(output))\n",
    "\n",
    "# Predict probability\n",
    "scores = clf_weighted.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "inflated_cf_matrix = confusion_matrix(y_true=y_test_inf, y_pred=predicted_y_test_inf)\n",
    "weighted_cf_matrix = confusion_matrix(y_true=y_test, y_pred=predicted_y_test_w)\n",
    "refined_cf_matrix = confusion_matrix(y_true=y_test, y_pred=predicted_y_test_ref)\n",
    "\n",
    "index = [\"setosa (act)\", \"versicolor (act)\", \"virginica (act)\"]\n",
    "columns = [\"setosa (pred)\", \"versicolor (pred)\", \"virginica (pred)\"]\n",
    "\n",
    "df_inflated = pd.DataFrame(inflated_cf_matrix, index, columns)\n",
    "df_weighted = pd.DataFrame(weighted_cf_matrix, index, columns)\n",
    "df_refined = pd.DataFrame(refined_cf_matrix, index, columns)\n",
    "\n",
    "captions = [\"Confusion Matrix - Inflated Tree\", \"Confusion Matrix - Weighted Tree\", \"Confusion Matrix - Refined Tree\"]\n",
    "\n",
    "# Pretty print the three confusion matrix\n",
    "display_side_by_side([df_inflated, df_weighted, df_refined], captions)"
   ]
  },
  {
   "source": [
    "## Step 5\n",
    "\n",
    "Build the ROC curves (or coverage curves in coverage space) and plot them for each tree model you have created: for each model you have to build three curves, one for each class, considered in turn as the positive class. (1-vs-rest model)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Initializing the configuratuion for the plots."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Save iris data\n",
    "X_plot = iris.data\n",
    "y_plot = iris.target\n",
    "\n",
    "# Binarize the output\n",
    "y_plot = label_binarize(y_plot, classes=[0, 1, 2])\n",
    "n_classes = y_plot.shape[1]\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X_plot, y_plot, test_size=0.20, random_state=7)\n",
    "\n",
    "# Subpolt configuration\n",
    "subplot_kw = dict(\n",
    "    xlabel=('False Positive Rate'),\n",
    "    ylabel=('True Positive Rate'),\n",
    "    xlim=([0.0, 1.0]),\n",
    "    ylim=([0.0, 1.05]))"
   ]
  },
  {
   "source": [
    "Defining some utility functions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "def calculate_actuals_binary(class_index, y_test):\n",
    "    \"\"\"\n",
    "    Calculates a binary array in a one vs many fashion (1 if the actual class corresponds to the index, 0 otherwise).\n",
    "    \"\"\"\n",
    "    # Actual is a vector containing 1 only if the correct class corresponds to the current class (determined by index i)\n",
    "    actual = np.zeros(y_test.shape[0])\n",
    "    for j in range(y_test.shape[0]):\n",
    "        if y_test[j] == class_index:\n",
    "            actual[j] = 1\n",
    "\n",
    "    return actual\n",
    "\n",
    " \n",
    "def calculate_scores():\n",
    "    \"\"\"\n",
    "    Appends the binary array for each \n",
    "    \"\"\"\n",
    "    result = arr = np.zeros([n_classes, y_test.shape[0]])\n",
    "    for i in range(n_classes):\n",
    "        array = calculate_actuals_binary(i, y_test)\n",
    "        result[i] = array\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# As seen in the documentation \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "def compute_roc(classifier, weighted: bool):\n",
    "    # Learn to predict each class against the other\n",
    "    # Differente procedure in case the clf has weight\n",
    "    if weighted:\n",
    "        y_train_binary = calculate_scores()\n",
    "        y_score_bin = classifier.predict_proba(X_test)\n",
    "    else:\n",
    "        classifier = OneVsRestClassifier(classifier)\n",
    "        y_score_bin = classifier.fit(X_train_bin, y_train_bin).predict_proba(X_test_bin)\n",
    "    # Predict each probability\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], thresholds = roc_curve(y_test_bin[:, i], y_score_bin[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    return (fpr, tpr, roc_auc)\n",
    "\n",
    "\n",
    "def plot_roc_curves(title: str, fpr, tpr, roc_auc):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(24, 6), subplot_kw=subplot_kw)\n",
    "    \"\"\"\n",
    "    Plot roc curves, one for each class of the datases.\n",
    "    \"\"\"\n",
    "\n",
    "    ax0.set_title('setosa (act)')\n",
    "    lw = 0\n",
    "    ax0.plot(fpr[lw], tpr[lw], color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[lw])\n",
    "    ax0.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax0.legend(loc=\"lower right\")\n",
    "\n",
    "    ax1.set_title('versicolor (act)')\n",
    "    lw = 1\n",
    "    ax1.plot(fpr[lw], tpr[lw], color='green', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[lw])\n",
    "    ax1.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    ax2.set_title('virginica (act)')\n",
    "    lw = 2\n",
    "    ax2.plot(fpr[lw], tpr[lw], color='purple', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[lw])\n",
    "    ax2.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "    fig.suptitle('ROC curves for {}'.format(title))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "source": [
    "### Plotting ROC curves\n",
    "\n",
    "#### Inflated case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Inflated case\n",
    "classifier: tree.DecisionTreeClassifier = clf_inflated\n",
    "title: str = \"Inflated Tree\"\n",
    "\n",
    "frp, tpr, roc_auc = compute_roc(classifier, False)\n",
    "plot_roc_curves(title, frp, tpr, roc_auc).show()"
   ]
  },
  {
   "source": [
    "#### Weighted case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Weigthed case\n",
    "classifier: tree.DecisionTreeClassifier = clf_weighted\n",
    "title: str = \"Weigthed Tree\"\n",
    "\n",
    "frp, tpr, roc_auc = compute_roc(classifier, True)\n",
    "plot_roc_curves(title, frp, tpr, roc_auc).show()"
   ]
  },
  {
   "source": [
    "#### Refined case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Refined case\n",
    "classifier: tree.DecisionTreeClassifier = clf_refined\n",
    "title: str = \"Refined Tree\"\n",
    "\n",
    "frp, tpr, roc_auc = compute_roc(classifier, False)\n",
    "plot_roc_curves(title, frp, tpr, roc_auc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}