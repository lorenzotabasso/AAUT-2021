{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('aaut': conda)",
      "metadata": {
        "interpreter": {
          "hash": "a07757eebbb88d9274b59226e18b77a2dd1d919e3b61c48eb9c7966fb3bb6670"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.8.5-final"
    },
    "colab": {
      "name": "classification_iris_knn_aa_20_21.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbORlHDOEYTE"
      },
      "source": [
        "# Classifiers comparison: decision trees and k-nearest neighbors on the dataset Iris\n",
        "\n",
        "\n",
        "In the following program we compare the prediction results obtained by decision trees and k-nearest neighbors on the dataset Iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "iFRSTXNDEYTG"
      },
      "source": [
        "The following cell shows the program training a decision tree and its results in preciction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common imports\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "from numpy import zeros\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Import for Decision Tree\n",
        "from sklearn import tree \n",
        "from sklearn.model_selection import cross_val_score # will be used to separate training and test\n",
        "\n",
        "# Import for Clustering\n",
        "from sklearn import neighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7fYOeG3EYTH",
        "outputId": "66a02d43-b277-4df9-fa0a-5448d89ff4b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "iris = load_iris()\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1})\n",
        "clf = clf.fit(iris.data, iris.target)\n",
        "scores = cross_val_score(clf, iris.data, iris.target, cv=5) # score will be the accuracy\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6Stp1gpEYTP"
      },
      "source": [
        "The following cell shows the training of k-nearest neighbors and its prediction results.\n",
        "Here we use a uniform weighting setting (weights='uniform'): any neighbors weights the same in the majority voting aggregation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZAavM5qEYTQ",
        "outputId": "eaf1d720-7bb6-4621-905e-09ab8974d5fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_neighbors = 11\n",
        "clf_knn = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\n",
        "clf_knn = clf_knn.fit(iris.data, iris.target)\n",
        "scores = cross_val_score(clf_knn, iris.data, iris.target, cv=5) # score will be the accuracy\n",
        "print(scores)\n",
        "# shows the model predictions  \n",
        "for i in range(len(iris.target)):\n",
        "    instance=(iris.data[i,:]).reshape(1, -1)\n",
        "    predicted=clf_knn.predict(instance)[0]\n",
        "    if iris.target[i]==predicted:\n",
        "        print(str(i)+\" ok \"+str(iris.target_names[iris.target[i]]))\n",
        "    else:\n",
        "        print(str(i)+\" nok \"+\"true class: \"+str(iris.target_names[iris.target[i]])+\"; predicted: \"+str(iris.target_names[predicted]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVI8abkeEYTV"
      },
      "source": [
        "In the following cell we use a varying weighting setting (weights='distance'): any neighbors weights inversely with its distance to the test instance in the majority voting aggregation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dxC1EypXEYTV",
        "outputId": "04a3ea00-6045-4507-e3c8-98bc124b4a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_neighbors = 11\n",
        "clf_knn2 = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
        "clf_knn2.fit(iris.data, iris.target)\n",
        "\n",
        "for i in range(len(iris.target)):\n",
        "    instance=(iris.data[i,:]).reshape(1, -1)\n",
        "    predicted2=clf_knn2.predict(instance)[0]\n",
        "    if iris.target[i]==predicted2:\n",
        "        print(str(i)+\" ok \"+str(iris.target_names[iris.target[i]]))\n",
        "    else:\n",
        "        print(str(i)+\" nok \"+\"true class: \"+str(iris.target_names[iris.target[i]])+\"; predicted: \"+str(iris.target_names[predicted]))\n",
        "print(\"Classification score of k-nn with distance weighting\")\n",
        "scores2 = cross_val_score(clf_knn2, iris.data, iris.target, cv=5,scoring='accuracy') # score will be the accuracy\n",
        "print(scores2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wpx4NW0EYTa"
      },
      "source": [
        "The following cell shows the tuning of the k-nn models with a varying value of k (number of nearest neighbors) and finds the best value of k (giving the maximum accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "-jhdpzK2EYTa",
        "outputId": "6eadc375-90cb-46bf-d6eb-237b942ff42f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "iris = load_iris()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
        "\n",
        "best_accuracy=0\n",
        "best_k=1\n",
        "A=np.zeros(len(y_train), dtype=np.float) # for storing accuracies\n",
        "for n_neighbors in np.arange(1,len(y_train)+1):\n",
        "    clf_knn3 = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
        "    # (n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
        "    clf_knn3.fit(X_train, y_train)\n",
        "    index=n_neighbors-1\n",
        "    A[index]=clf_knn3.score(X_test, y_test)\n",
        "    if best_accuracy<clf_knn3.score(X_test, y_test):\n",
        "        best_accuracy=clf_knn3.score(X_test, y_test)\n",
        "        best_k=n_neighbors\n",
        "    print(\"k neighbors=\"+str(n_neighbors))\n",
        "    print(\"accuracy=\"+str(clf_knn3.score(X_test, y_test)))\n",
        "    \n",
        "print(\"\\n\")\n",
        "print(\"best k=\"+str(best_k))\n",
        "print(\"best accuracy=\"+str(best_accuracy))\n",
        "\n",
        "#plt.xticks(np.arange(1, len(y_train)+1, 8))\n",
        "plt.yticks(np.arange(0.0,1.0,0.01))\n",
        "\n",
        "plt.plot(np.arange(1,len(y_train)+1),A)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "p23LUav0EYTe"
      },
      "source": [
        "In the following cell we plot in the same plot two subplots with the diagrams on accuracy with the two kinds of weighting \n",
        "of the vote of the neighbours (uniform and with distance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewb11oFOEYTf",
        "outputId": "a64d135d-b84c-4051-f1b6-5cd248b5c70f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "iris = load_iris()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
        "\n",
        "i=0  #parameter in the control of the subplot to draw on'\n",
        "f,(ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "for weight_type in ['uniform','distance']:\n",
        "    print(\"weighting:\"+str(weight_type))\n",
        "    A=np.zeros(len(y_train), dtype=np.float) # for storing accuracies\n",
        "    best_accuracy=0\n",
        "    best_k=1\n",
        "    for n_neighbors in np.arange(1,len(y_train)+1):\n",
        "        clf_knn2 = neighbors.KNeighborsClassifier(n_neighbors, weights=weight_type)\n",
        "        # (n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
        "        clf_knn2.fit(X_train, y_train)\n",
        "        index=n_neighbors-1\n",
        "        A[index]=clf_knn2.score(X_test, y_test)\n",
        "        if best_accuracy<clf_knn2.score(X_test, y_test):\n",
        "            best_accuracy=clf_knn2.score(X_test, y_test)\n",
        "            best_k=n_neighbors\n",
        "        print(\"k neighbors=\"+str(n_neighbors))\n",
        "        print(\"accuracy=\"+str(clf_knn2.score(X_test, y_test)))\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    print(\"best k=\"+str(best_k))\n",
        "    print(\"best accuracy=\"+str(best_accuracy))\n",
        "    if i==0:\n",
        "        ax1.plot(np.arange(1,len(y_train)+1),A)\n",
        "        ax1.set_title('weighting type:'+str(weight_type))\n",
        "    else:\n",
        "        ax2.plot(np.arange(1,len(y_train)+1),A)\n",
        "        ax2.set_title('weighting type:'+str(weight_type))\n",
        "    i=i+1\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "LXhkXpz0EYTj"
      },
      "source": [
        "In the following cell we plot (overlapping) in the same picture both the diagrams on accuracy with the two kinds of weighting \n",
        "of the vote of the neighbours (uniform and with distance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_nkikvMEYTk",
        "outputId": "e5fb0243-2767-4448-e9b5-7252c8bf1610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "iris = load_iris()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Accuracy in k-nn with number of neighbors and types of weighting', fontsize=14, fontweight='bold')\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlabel('n. neighbors')\n",
        "ax.set_ylabel('accuracy')\n",
        "\n",
        "A=np.zeros((len(y_train),2), dtype=np.float) # 2 arrays for storing accuracies for each type of weigthing\n",
        "i=0  #parameter in the control of the different diagram (=matrix A column index)\n",
        "best_accuracy=0\n",
        "for weight_type in ['uniform','distance']:\n",
        "    print(\"\\n weighting:\"+str(weight_type))\n",
        "    best_accuracy=0\n",
        "    best_k=1\n",
        "    for n_neighbors in np.arange(1,len(y_train)+1):\n",
        "        clf_knn2 = neighbors.KNeighborsClassifier(n_neighbors, weights=weight_type)\n",
        "        # (n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
        "        clf_knn2.fit(X_train, y_train)\n",
        "        index=n_neighbors-1 # computes the matrix row index\n",
        "        A[index,i]=clf_knn2.score(X_test, y_test)\n",
        "        if best_accuracy<clf_knn2.score(X_test, y_test):\n",
        "            best_accuracy=clf_knn2.score(X_test, y_test)\n",
        "            best_k=n_neighbors\n",
        "        print(\"k neighbors=\"+str(n_neighbors))\n",
        "        print(\"accuracy=\"+str(clf_knn2.score(X_test, y_test)))\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    print(\"best k=\"+str(best_k))\n",
        "    print(\"best accuracy=\"+str(best_accuracy))\n",
        "    plt.plot(np.arange(1,len(y_train)+1),A[:,i])\n",
        "    i=i+1\n",
        "plt.legend(['uniform', 'distance'], loc='lower left')  \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "-aSX54vFEYTp"
      },
      "source": [
        "Plot the Iris dataset, in 2-D, with a red color for Setosa, blu for Versicolor, Green for Virginica.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o8cPLOdEYTq"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs9ogv4vEYTt"
      },
      "source": [
        "Plot the Iris dataset, in 2-D, with the color as above determined by the k-nn estimation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4d3We4cEYTu"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqQOXvLKEYTy"
      },
      "source": [
        "In the following, extend the above exercize on k-nn using a kernel function  \n",
        "K(x,y) for the distances computation, such that distance(x,y)=1-K(x,y).    Use a Gaussian-like (or Radial Basis Function) kernel K(x,y)=exp(-gamma(x-y)^2), with gamma the inverse of the sigma squared variance, that must be tuned to the best value according to the accuracy reached by the k-nn, with k=7 (similarly as done with the previous example on tuning the parameter n_neightbors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBWkBiriEYTy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}